# Research-Methods-and-Professional-Practice
# E - Portfolio
## About me 
I am a Masters student pursuing Data science based in the UAE.


## Skills 
- Data Anlysis
- Business analytics
- Data Visualization
- Machine Learning Algorithms
- Deep Learning

## Expertise 
Python, R, Google Analytics 4, Tableau, power BI, Jira, SQLplus , MySQL, SAP 

## Projects
Here are all the tasks of the unit Research Methods and Professional Practice so far!

### Introduction 
This unit gave me the chance to really understand how research fits into the world of data science and not just as a process, but as a way of thinking. As I worked through different topics like building research questions, analysing data, and considering ethical issues, I started to see research as something much more than theory. The tasks we did like case studies, reflections, and proposal writing helped me link ideas to real practice. I now view research as a practical tool for asking better questions, making informed decisions, and being more accountable in how I work with data, especially when the outcomes affect people in the real world.

### How I’ve applied these concepts in practice
The activities paired with self reflection really made me aware of where I stood in terms of my personal development.

### Unit 1
#### Reflective Activity 1 – Ethics in Computing in the age of Generative AI
Since the end of 2022, it feels like AI especially generative AI has gone from something we occasionally read about to something we interact with almost daily. Chatbots, image generators, AI-assisted coding and it has all become very normal and very fast. At first, I was impressed but the more I looked at how it is being used, the more I realised there are real gaps in how we are handling the responsibility side of things. As a Data Science student, this realisation hit me harder than I expected. It made me question not only where AI is heading but what role people like me will play in shaping that future.
Reading Correa et al. (2023) helped me see the big picture. Their review of 200 different AI ethics documents from around the world shows how much work is going into defining what “ethical AI” actually means. The surprising part was that there as a lot of overlap in the values and things like transparency, fairness, and accountability come up all the time. But the interpretation of those values varies wildly. Some countries or institutions define transparency as open-source code. Others see it as explainable decisions. Some barely define it at all.
That kind of inconsistency might not seem like a big deal at first but it has consequences. Without shared definitions, we end up with policies that sound responsible on the surface but don’t always lead to meaningful action. Correa and colleagues point this out clearly. Many of these documents are more about signalling than substance. They offer broad ethical goals but few come with clear tools or standards for implementation.
At the same time, Deckard (2023) offers a more grounded view of AI ethics. The way he writes about it is more practical focused on the everyday decisions developers and teams must make. What stood out most to me was his emphasis that ethics is not a box you tick once at the beginning of a project. It is an ongoing responsibility. You do not just write a fairness statement and move on. You keep asking questions even when it is inconvenient.
The challenge is that, as Correa et al. (2023) makes it clear that there is no global roadmap for AI governance. Countries are doing their own thing and in many cases the rules are vague or still being written. That means computing professionals are often left to fill in the ethical blanks themselves. But the hopeful part is that we are not starting from scratch. We have guidance even if it is a bit fragmented. And more importantly, we have the ability to shape what ethical practice looks like in our own environments.
In order to do that three things come in my mind.
First, we need clarity on what ethical terms mean in practice. Words like “transparency” or “harm reduction” should come with examples, case studies, and tools and not just buzzwords. I am not saying every country needs to agree on every detail. But there needs to be enough common ground that we are not reinventing the wheel or talking past each other.
Second, ethics should be part of the design and development process, not something tagged on at the end. Deckard (2023) is clear about this: ethical questions should be asked during system development and not just when things go wrong. That might mean setting up review checkpoints or having cross disciplinary teams look at unintended consequences. It is about creating room to pause and think even when the pressure to launch quickly is high.
Third, we need to reframe what professional responsibility looks like in computing. It is not just about writing clean, functional code. It is about owning the impact of that code, even after it is deployed. It means moving away from the idea that “ethics is someone else’s job.” As a student, I used to assume that too surely there is a legal team or a compliance officer who handles the hard stuff. But now I realised, those decisions often start earlier with the design choices we make as developers.
I also think it is important to be honest and being ethical is not always easy. It can feel awkward to question your team’s approach or point out a problem others have not noticed. But as Deckard (2023) suggests, ethics is not about perfection. It is about showing up, staying curious, and being willing to change course when needed.

In conclusion, AI governance is not just about law or policy it is about people. And while international frameworks like those reviewed by Correa et al. (2023) are essential they only go so far without everyday professionals putting those values into practice. Ethics does not happen in strategy documents. It happens in meetings and code reviews.
Going forward, I do not think we need one universal AI rulebook. But we do need common reference points, clearer definitions, and a stronger culture of responsibility within the computing field.

References
Correa, N., Soler, C., Presno, M.J. and Esponda, I., 2023. Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance.
Deckard, R., 2023. What are Ethics in AI. British Computer Society. Available at: https://www.bcs.org/articles-opinion-and-research/what-are-ethics-in-ai/ 


#### Collaborative Learning Discussion 1
The 'Abusive Workplace Behavior' case sheds light on the significant impact toxic leadership can have on both team morale and individual well being. Diane was recruited for her impressive academic track record but quickly became the target of Max who was the team’s technical lead. After a minor coding mistake, Max's response which was shouting at Diane and excluding her from a crucial public demo was harsh but it also fits a troubling pattern. Women on the team have been subjected to similar treatment including having their names removed from journal submissions as a form of punishment.
When Diane approached her manager for support she was met with indifference and was told to “grow up.”. Jean's dismissal of the situation not only undermines Diane’s concerns but also perpetuates a work culture where abusive behavior is tolerated. This violates multiple key principles in the ACM Code of Ethics such as 1.1 (human well-being) and 1.4 (non-discrimination) (ACM, 2018). Jean’s failure to act reflects poorly on leadership and shows a lack of accountability and violates the principles 3.3 and 3.4, which highlights the importance of supporting ethical environments and addressing workplace concerns. Similarly, the BCS Code of Conduct underscores the need for integrity, professionalism, and safeguarding team members, all of which are neglected in this case (BCS, 2015).
This case serves as a powerful reminder that leadership must go beyond technical ability.

References:
ACM (2018). Abusive Workplace Behavior. https://www.acm.org/code-of-ethics/case-studies/abusive-workplace-behavior
BCS (2015). BCS Code of Conduct. https://www.bcs.org/membership/become-a-member/bcs-code-of-conduct/

### Unit 2
#### Literature Review and Research Proposal Outlines
Overview of the Topic:
The gender pay gap is a widely recognised issue across many industries. This proposal focuses on understanding how it manifests specifically within the UK’s technology sector.

Rationale:
The tech industry is a fast-growing and highly influential field. Exploring pay inequality in this context can provide useful insights for improving fairness and inclusion.

Aim of the Study:
To explore the causes and implications of gender-based pay differences in the UK tech industry and to identify possible ways to address them.

Research Questions
What are the main factors contributing to the gender pay gap in the UK tech sector?
How do company structures and workplace culture influence pay differences?
What steps can organisations take to reduce the gap?

Objectives
To investigate the current state of pay equality between men and women in UK tech industry.
To understand employee experiences and perceptions regarding pay and career progression.
To suggest strategies for reducing the gender pay gap in the industry.

Approach:
A combination of quantitative and qualitative research methods will be used.

Data Collection:
Quantitative data (e.g., salary data, job roles, gender distribution).
Qualitative data (e.g., interviews or surveys with tech employees).

Analysis:
Data will be examined to identify patterns, trends, and themes related to gender pay disparities.

Ethical Considerations:
Participants will be informed about the research purpose.
Confidentiality and voluntary participation will be ensured.

Expected Outcomes
A general overview of the gender pay gap in the UK tech sector.
Better understanding of workplace factors that influence pay equity.
Practical suggestions for organisations and professionals in the industry.

### Unit 3
#### Research Proposal Review

A combination of quantitative and qualitative research methods will be used.

Data Collection:
Quantitative data (e.g., salary data, job roles, gender distribution).
Qualitative data (e.g., interviews or surveys with tech employees).

### Unit 5
#### Reflective Activity 2
Case Study: Inappropriate Use of Surveys
Looking back at the Cambridge Analytica incident, it is easy to see how something as familiar and seemingly harmless as an online quiz could lead to one of the most talked about data misuse cases in recent years. What started as a few light-hearted surveys on Facebook ended up exposing the personal information of millions of users many of whom had no idea they were even involved.

The surveys themselves did not seem unusual. They asked fun questions often disguised as personality tests. People shared their answers out of curiosity or boredom, probably never imagining it would go further than that. But what made the situation so concerning was how the data behind the scenes was handled. By completing the surveys users gave permission sometimes unknowingly for the app to access more than just their answers. In many cases, it pulled in information from their profiles including data from their friends accounts too.

That collected data was then analysed and used to build detailed psychological and behavioural profiles of users. These profiles were reportedly used to help shape political campaigns by targeting people with specific messages that matched their personality types or beliefs. This kind of microtargeting blurred the lines between persuasion and manipulation.

The real issue here was not just about data being collected but it was about the lack of awareness and consent. Most people did not know their information or their friend’s information was being taken and used in this way. There was not enough transparency and the original purpose of the surveys was completely different from how the data was eventually used. That disconnect between intention and outcome is what made it feel like a breach of trust.

For me, this case really shows how easy it is for a familiar tool like a survey to be turned into something more powerful and potentially dangerous when data is involved. It is not just about asking questions it is about what is being done with the answers. It also reminds me how important it is for anyone designing research tools to be upfront about their intentions and to think beyond the short-term goals. Just because a method is technically allowed does not mean it is ethically right.

In the end, what happened with Cambridge Analytica was not just about one company or one platform it was about how much responsibility we all carry when it comes to collecting and handling data. And it is a reminder that in digital spaces, even something as ordinary as a quiz can have much bigger implications than expected.

### Unit 7
#### Hypothesis Testing Worksheet

The Related Samples T Test
<img width="969" height="737" alt="image" src="https://github.com/user-attachments/assets/ef4a50d8-5259-4b12-9ed5-2dc635de6753" />

The INDEPENDENT Samples T Test
<img width="736" height="727" alt="image" src="https://github.com/user-attachments/assets/87f1d5c5-e616-41b6-b3a9-28ea2632828c" />

### Unit 8
#### Charts Worksheet
Bar Charts in Excel
<img width="1388" height="795" alt="image" src="https://github.com/user-attachments/assets/1b625a7a-18f4-47a2-856c-119b795f0ab6" />

Histograms
<img width="1083" height="798" alt="image" src="https://github.com/user-attachments/assets/81b853fb-904a-42b7-a568-14d9ae573b85" />

